{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import all packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import skorch.callbacks.base\n",
    "\n",
    "import osgit\n",
    "import sys\n",
    "sys.path.insert(0, 'adamwr') # you will need to have adamW optimizer cloned locally\n",
    "sys.path.insert(0, 'cgcnn/')\n",
    "import cgcnn\n",
    "import mongo\n",
    "\n",
    "from cgcnn.data import collate_pool, MergeDataset, StructureDataTransformer\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skorch.callbacks import Checkpoint, LoadInitState \n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from cosine_scheduler import CosineLRWithRestarts\n",
    "from adamw import AdamW\n",
    "\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the cleavage energy docs and convert the structures into graph objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pickle.load(open('../cleavage_energy_dataset/intermetallics_cleavage_energy_data.pkl' ,'rb'))\n",
    "random.seed(123)\n",
    "random.shuffle(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    doc[\"atoms\"] = doc['thinnest_structure']['atoms']\n",
    "    doc[\"results\"] = doc['thinnest_structure']['results']\n",
    "    doc[\"initial_configuration\"] = doc['thinnest_structure']['initial_configuration']\n",
    "    del doc[\"thinnest_structure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDT = StructureDataTransformer(atom_init_loc='/home/zulissi/software/cgcnn_sklearn/atom_init.json',\n",
    "                              max_num_nbr=12,\n",
    "                               step=0.8,\n",
    "                              radius=4,\n",
    "                              use_voronoi=False,\n",
    "                              use_tag=False,\n",
    "                              use_fixed_info=False,\n",
    "                              use_distance=False,\n",
    "                              train_geometry = 'initial'\n",
    "                              )\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "structures = SDT_out[0]\n",
    "\n",
    "#Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "with mp.Pool(4) as pool:\n",
    "    SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare prediction labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = np.array([[int(docs.index(doc)), np.log(doc['surface_energy'])] for doc in docs])\n",
    "target_list = pd.DataFrame(target_list, columns = ['doc_index', 'surface_energy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into 80:20 train:test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDT_training, SDT_test, target_training, target_test = train_test_split(SDT_list, target_list, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the model and train the model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further spilt the training data into train and validate set by 8:2 ratio to avoid overfitting\n",
    "train_test_splitter = ShuffleSplit(test_size=0.2, random_state=42)\n",
    "LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=87, epoch_size=len(SDT_training), restart_period=10, t_mult=1.2)\n",
    "\n",
    "class MyNet(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, **kwargs):\n",
    "        y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "        return super().get_loss(y_pred, y_true, **kwargs)\n",
    "\n",
    "## below is the sigopt best assignment\n",
    "net = MyNet(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=87,  \n",
    "    module__classification=False,\n",
    "    lr=np.exp(-6.465085550816676),     \n",
    "    max_epochs=300,\n",
    "    module__atom_fea_len=43,\n",
    "    module__h_fea_len=114,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=3, \n",
    "    module__use_distance=False,\n",
    "    module__cutoff=100,\n",
    "    optimizer=AdamW,\n",
    "    optimizer__weight_decay=1e-2,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_train__shuffle=True, #VERY IMPORTANT\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    iterator_valid__shuffle=False, #This should be False, which is the default\n",
    "    device=device,\n",
    "   criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")\n",
    "\n",
    "net.initialize()\n",
    "net.fit(SDT_training,np.array(target_training[['surface_energy']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions and visualize the predictions with parity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {'doc_index': list(target_training['doc_index']),\n",
    "                 'type': 'train', \n",
    "                 'actual_value':np.exp(target_train['surface_energy']),\n",
    "                 'predicted_value':np.exp(net.predict(SDT_train).reshape(-1))}\n",
    "\n",
    "test_data = {'doc_index': list(target_test['doc_index']),\n",
    "             'type': 'test',\n",
    "            'actual_value':np.exp(target_test['surface_energy']),\n",
    "            'predicted_value':np.exp(net.predict(SDT_test).reshape(-1))}\n",
    "\n",
    "\n",
    "df_training = pd.DataFrame(training_data)\n",
    "df_test = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter(df_training['actual_value'], df_training['predicted_value'], color='yellowgreen', \n",
    "           marker='o', alpha=0.5, label='train: MAE=%0.4f eV/$\\AA^2$, RMSE=%0.3f eV/$\\AA^2$'\\\n",
    "            %(mean_absolute_error(df_training['actual_value'], df_training['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_training['actual_value'], df_training['predicted_value']))))\n",
    "\n",
    "ax.scatter(df_test['actual_value'], df_test['predicted_value'], color='cornflowerblue', \n",
    "           marker='o', alpha=0.5, label='test: MAE=%0.4f eV/$\\AA^2$, RMSE=%0.3f eV/$\\AA^2$'\\\n",
    "            %(mean_absolute_error(df_test['actual_value'], df_test['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_test['actual_value'], df_test['predicted_value']))))\n",
    "\n",
    "ax.plot([min(df_training['actual_value']), max(df_training['actual_value'])-0.25], \n",
    "        [min(df_training['actual_value']), max(df_training['actual_value'])-0.25], 'k--')\n",
    "\n",
    "# format graph\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.set_xlabel('DFT Energy (eV/$\\AA^2$)', fontsize=20)\n",
    "ax.set_ylabel('CGCNN predicted Energy (eV/$\\AA^2$)', fontsize=20)\n",
    "ax.set_xlim(0,0.35)\n",
    "ax.set_ylim(0,0.35)\n",
    "#ax.set_title('Multi-element ', fontsize=14) \n",
    "ax.legend(fontsize=15, loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get atomic contributions trajectories\n",
    "\n",
    "We picked the ones with reasonably accurate prediction as an example, but you can loop through the test data index and make trajectories of atomic contriution for all test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_idx = np.where(np.array((abs(df_test['actual_value'] - df_test['predicted_value']))) < 0.00005)[0]\n",
    "\n",
    "for idx in visual_idx:\n",
    "    doc_idx = int(df_test.iloc[idx]['doc_index']) \n",
    "    out, atom_fea = net.forward([SDT_list[doc_idx]])\n",
    "    contributions = atom_fea.cpu().data.numpy().reshape(-1)\n",
    "    atoms = mongo.make_atoms_from_doc(docs[doc_idx])\n",
    "    atoms.set_initial_charges(np.exp(contributions))\n",
    "    atoms.write('./Traj/docs_%d.traj'%(doc_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
